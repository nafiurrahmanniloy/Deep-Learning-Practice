{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1a2fe3-2f7c-456a-a165-7fa6b56850dd",
   "metadata": {},
   "source": [
    "***Import Required Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a594a008-089f-453d-9010-da319bdab62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd991741-2ea7-417d-910e-fd266899ca38",
   "metadata": {},
   "source": [
    "***Set Up Dataset Directory Structure***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cd902-c35b-44ac-a289-1aa86419b1b6",
   "metadata": {},
   "source": [
    "*This section prepares the folder structure for training by merging all class images into a unified training folder grouped by label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54d63599-154d-4383-91ab-71940a8a3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'Data'\n",
    "defective_dir = os.path.join(base_dir, 'Defective_bottle')\n",
    "proper_dir = os.path.join(base_dir, 'Proper_bottle')\n",
    "cans_dir = os.path.join(base_dir, 'train_cans')\n",
    "\n",
    "unified_train_dir = 'UnifiedTrain'\n",
    "shutil.rmtree(unified_train_dir, ignore_errors=True)\n",
    "os.makedirs(unified_train_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(unified_train_dir, 'Defective'), exist_ok=True)\n",
    "os.makedirs(os.path.join(unified_train_dir, 'Proper'), exist_ok=True)\n",
    "os.makedirs(os.path.join(unified_train_dir, 'Can'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617fdcc-3e2d-4707-a2c3-83c33c5ea0af",
   "metadata": {},
   "source": [
    "***Copy Images into Unified Directory***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213683e8-0911-4cf9-a10d-d3446c5f4117",
   "metadata": {},
   "source": [
    "*This function copies images from the original folders into the appropriate subdirectories within UnifiedTrain. It ensures that the dataset is correctly structured for the ImageDataGenerator.*|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3337f0c2-7eeb-49e4-9971-787c4b1377c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(src_dir, dst_dir):\n",
    "    for img_name in os.listdir(src_dir):\n",
    "        src_path = os.path.join(src_dir, img_name)\n",
    "        if os.path.isfile(src_path):\n",
    "            shutil.copy(src_path, dst_dir)\n",
    "\n",
    "copy_images(defective_dir, os.path.join(unified_train_dir, 'Defective'))\n",
    "copy_images(proper_dir, os.path.join(unified_train_dir, 'Proper'))\n",
    "copy_images(cans_dir, os.path.join(unified_train_dir, 'Can'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef10b7-09d3-4890-a387-a21d76d8110a",
   "metadata": {},
   "source": [
    "***Create Data Generators for Training and Validation***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ab09c-1d1e-4d76-b481-85a5c840dd11",
   "metadata": {},
   "source": [
    "*Here we create an ImageDataGenerator that normalizes image pixel values and splits the dataset into training and validation subsets.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cb6f2d3-c966-4c7e-9d92-eb33f2fb6900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 770 images belonging to 3 classes.\n",
      "Found 192 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    unified_train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = train_gen.flow_from_directory(\n",
    "    unified_train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5894ae5-94ac-414d-af6c-ea0e1e7e9a9c",
   "metadata": {},
   "source": [
    "***Build the Transfer Learning Model with MobileNetV2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051e9e4-97aa-434a-aa31-2d437fe17a6f",
   "metadata": {},
   "source": [
    "*We initialize MobileNetV2 without its top layers and freeze its weights. Then, we stack new classification layers to adapt it for our 3-class bottle classification task.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "145b7205-8cb6-4129-8fec-30a252a3b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11eed8a-9439-4049-b129-8c70a16db98d",
   "metadata": {},
   "source": [
    "***Train the Model with Callbacks***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c30736-7649-4052-bfd9-d7f9c5ee358f",
   "metadata": {},
   "source": [
    "*Train the model using training data and monitor validation accuracy.\n",
    " Save the best model and stop early if accuracy stops improving*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e9851ff-abb0-47ec-9b4f-5cf74babea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafiu\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.3595 - loss: 1.3588"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 473ms/step - accuracy: 0.3612 - loss: 1.3536 - val_accuracy: 0.5052 - val_loss: 0.8794\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.6386 - loss: 0.8366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 331ms/step - accuracy: 0.6402 - loss: 0.8341 - val_accuracy: 0.8646 - val_loss: 0.5531\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.7954 - loss: 0.5778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 332ms/step - accuracy: 0.7961 - loss: 0.5763 - val_accuracy: 0.8906 - val_loss: 0.4080\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.8444 - loss: 0.4302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 338ms/step - accuracy: 0.8452 - loss: 0.4300 - val_accuracy: 0.9219 - val_loss: 0.3430\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8696 - loss: 0.3706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 331ms/step - accuracy: 0.8693 - loss: 0.3707 - val_accuracy: 0.9375 - val_loss: 0.2909\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 322ms/step - accuracy: 0.9116 - loss: 0.2868 - val_accuracy: 0.9375 - val_loss: 0.2544\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.9133 - loss: 0.2881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 340ms/step - accuracy: 0.9135 - loss: 0.2874 - val_accuracy: 0.9479 - val_loss: 0.2287\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 333ms/step - accuracy: 0.9079 - loss: 0.2581 - val_accuracy: 0.9479 - val_loss: 0.2120\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9144 - loss: 0.2566"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 331ms/step - accuracy: 0.9143 - loss: 0.2567 - val_accuracy: 0.9583 - val_loss: 0.1985\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9304 - loss: 0.2347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 330ms/step - accuracy: 0.9302 - loss: 0.2342 - val_accuracy: 0.9635 - val_loss: 0.1927\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 329ms/step - accuracy: 0.9305 - loss: 0.1989 - val_accuracy: 0.9427 - val_loss: 0.1783\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 326ms/step - accuracy: 0.9348 - loss: 0.1846 - val_accuracy: 0.9479 - val_loss: 0.1811\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 325ms/step - accuracy: 0.9257 - loss: 0.1829 - val_accuracy: 0.9427 - val_loss: 0.1745\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 334ms/step - accuracy: 0.9592 - loss: 0.1443 - val_accuracy: 0.9583 - val_loss: 0.1653\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 426ms/step - accuracy: 0.9450 - loss: 0.1554 - val_accuracy: 0.9427 - val_loss: 0.1644\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('bottle_can_classifier.h5', monitor='val_accuracy', save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a7508-15d9-4e4f-a722-f831113dda21",
   "metadata": {},
   "source": [
    "***Evaluate the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62e43966-b061-4b43-8d95-ddc1dbc4ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 296ms/step - accuracy: 0.9711 - loss: 0.1739\n",
      "Validation Loss: 0.19271768629550934\n",
      "Validation Accuracy: 0.9635416865348816\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c5356-6094-4229-85cd-d612c3c4bc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
